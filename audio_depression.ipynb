{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d1cb156",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import librosa\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b45aef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_PATH: D:\\depressiondetector\\diag-woz\n",
      "OUTPUT_CSV_FILE: D:\\depressiondetector\\extracted_features\\audio_features.csv\n"
     ]
    }
   ],
   "source": [
    "# === EDIT THESE PATHS BEFORE RUNNING ===\n",
    "DATASET_PATH = r\"D:\\depressiondetector\\diag-woz\"   \n",
    "LABEL_DIR = r\"D:\\depressiondetector\"      \n",
    "# Name for extracted features CSV (will be written inside DATASET_PATH or another location)\n",
    "OUTPUT_DIR = r\"D:\\depressiondetector\\extracted_features\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "OUTPUT_CSV_FILE = r\"D:\\depressiondetector\\extracted_features\\audio_features.csv\"\n",
    "\n",
    "print(\"DATASET_PATH:\", DATASET_PATH)\n",
    "print(\"OUTPUT_CSV_FILE:\", OUTPUT_CSV_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83a23a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels: (163, 6)\n",
      "Dev labels:   (56, 6)\n",
      "Test labels:  (56, 6)\n",
      "train columns: ['Participant_ID', 'Gender', 'PHQ_Binary', 'PHQ_Score', 'PCL-C (PTSD)', 'PTSD Severity']\n"
     ]
    }
   ],
   "source": [
    "# Paths to splits (update names if your files differ)\n",
    "train_labels_path = r\"D:\\depressiondetector\\labels\\train_split.csv\"\n",
    "dev_labels_path   = r\"D:\\depressiondetector\\labels\\dev_split.csv\"\n",
    "test_labels_path  = r\"D:\\depressiondetector\\labels\\test_split.csv\"  # or full_test_split.csv if that's your filename\n",
    "\n",
    "# Load safely with checks\n",
    "for p in (train_labels_path, dev_labels_path, test_labels_path):\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Required label file not found: {p}\")\n",
    "\n",
    "train_labels = pd.read_csv(train_labels_path)\n",
    "dev_labels   = pd.read_csv(dev_labels_path)\n",
    "test_labels  = pd.read_csv(test_labels_path)\n",
    "\n",
    "print(\"Train labels:\", train_labels.shape)\n",
    "print(\"Dev labels:  \", dev_labels.shape)\n",
    "print(\"Test labels: \", test_labels.shape)\n",
    "\n",
    "# Inspect columns to find the target column name(s)\n",
    "print(\"train columns:\", train_labels.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "944b0e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train columns: ['Participant_ID', 'Gender', 'PHQ8_Binary', 'PHQ_Score', 'PCL-C (PTSD)', 'PTSD Severity']\n",
      "dev columns: ['Participant_ID', 'Gender', 'PHQ8_Binary', 'PHQ_Score', 'PCL-C (PTSD)', 'PTSD Severity']\n",
      "test columns: ['Participant_ID', 'Gender', 'PHQ8_Binary', 'PHQ_Score', 'PCL-C (PTSD)', 'PTSD Severity']\n"
     ]
    }
   ],
   "source": [
    "# Different CSVs sometimes use different label column names. Normalize to 'PHQ8_Binary'\n",
    "def normalize_label_column(df):\n",
    "    # common variants observed: 'PHQ8_Binary', 'PHQ_Binary', 'PHQ8_binary', etc.\n",
    "    candidates = ['PHQ8_Binary','PHQ_Binary','PHQ8_binary','PHQ_binary','phq8_binary','PHQ8']\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            df = df.rename(columns={c:'PHQ8_Binary'})\n",
    "            break\n",
    "    # If still not present, try to find something with \"PHQ\" substring.\n",
    "    if 'PHQ8_Binary' not in df.columns:\n",
    "        for c in df.columns:\n",
    "            if 'PHQ' in c.upper():\n",
    "                df = df.rename(columns={c:'PHQ8_Binary'})\n",
    "                break\n",
    "    return df\n",
    "\n",
    "train_labels = normalize_label_column(train_labels)\n",
    "dev_labels   = normalize_label_column(dev_labels)\n",
    "test_labels  = normalize_label_column(test_labels)\n",
    "\n",
    "# Ensure Participant_ID column exists under consistent name:\n",
    "for df_name, df in [('train', train_labels), ('dev', dev_labels), ('test', test_labels)]:\n",
    "    if 'Participant_ID' not in df.columns and 'participant_ID' in df.columns:\n",
    "        df.rename(columns={'participant_ID':'Participant_ID'}, inplace=True)\n",
    "    print(df_name, \"columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9dc13e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique participants across splits: 275\n"
     ]
    }
   ],
   "source": [
    "# Create a unified list of unique participant IDs across splits\n",
    "train_ids = train_labels['Participant_ID'].astype(str).tolist()\n",
    "dev_ids   = dev_labels['Participant_ID'].astype(str).tolist()\n",
    "test_ids  = test_labels['Participant_ID'].astype(str).tolist()\n",
    "\n",
    "all_participant_ids = sorted(list(set(train_ids + dev_ids + test_ids)))\n",
    "print(f\"Total unique participants across splits: {len(all_participant_ids)}\")\n",
    "# all_participant_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe957cca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300', '301', '302', '303', '304', '305', '306', '307', '308', '309']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_participant_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "423682d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['302', '303', '304', '305', '307', '308', '309', '310', '311', '312']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82087516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['300', '301', '306', '317', '320', '321', '331', '334', '336', '343']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_ids[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc003961",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad9cd35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_stats(arr):\n",
    "    \"\"\"Return mean, std, min, max for a numpy array (flattened).\"\"\"\n",
    "    arr_flat = np.asarray(arr).flatten()\n",
    "    if arr_flat.size == 0:\n",
    "        return (np.nan, np.nan, np.nan, np.nan)\n",
    "    return (np.mean(arr_flat), np.std(arr_flat), np.min(arr_flat), np.max(arr_flat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6304584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features_from_file(file_path, sr_target=16000, duration=180):\n",
    "    \"\"\"\n",
    "    Extract enhanced features with prosody (depression-specific)\n",
    "    duration: 180s = 3 minutes (good balance)\n",
    "    \"\"\"\n",
    "    y, sr = librosa.load(file_path, sr=sr_target, duration=duration)\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "    feats = {}\n",
    "    \n",
    "    # ===== EXISTING FEATURES (IMPROVED) =====\n",
    "    # MFCC (20 instead of 13 - captures more articulation detail)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=20)\n",
    "    feats['mfcc_mean'], feats['mfcc_std'], feats['mfcc_min'], feats['mfcc_max'] = aggregate_stats(mfccs)\n",
    "    \n",
    "    # Chroma\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    feats['chroma_mean'], feats['chroma_std'], feats['chroma_min'], feats['chroma_max'] = aggregate_stats(chroma)\n",
    "    \n",
    "    # Mel spectrogram\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    feats['mel_mean'], feats['mel_std'], feats['mel_min'], feats['mel_max'] = aggregate_stats(mel)\n",
    "    \n",
    "    # Spectral contrast\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    feats['contrast_mean'], feats['contrast_std'], feats['contrast_min'], feats['contrast_max'] = aggregate_stats(contrast)\n",
    "    \n",
    "    # Tonnetz\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    feats['tonnetz_mean'], feats['tonnetz_std'], feats['tonnetz_min'], feats['tonnetz_max'] = aggregate_stats(tonnetz)\n",
    "    \n",
    "    # ===== NEW: PROSODY FEATURES (CRITICAL FOR DEPRESSION) =====\n",
    "    # 1. Pitch (F0) - depression reduces pitch variability\n",
    "    try:\n",
    "        f0 = librosa.yin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "        f0_valid = f0[~np.isnan(f0)]\n",
    "        if len(f0_valid) > 0:\n",
    "            feats['pitch_mean'] = np.mean(f0_valid)\n",
    "            feats['pitch_std'] = np.std(f0_valid)\n",
    "            feats['pitch_range'] = np.max(f0_valid) - np.min(f0_valid)\n",
    "        else:\n",
    "            feats['pitch_mean'] = 0\n",
    "            feats['pitch_std'] = 0\n",
    "            feats['pitch_range'] = 0\n",
    "    except:\n",
    "        feats['pitch_mean'] = 0\n",
    "        feats['pitch_std'] = 0\n",
    "        feats['pitch_range'] = 0\n",
    "    \n",
    "    # 2. Energy (RMS) - depression reduces vocal energy\n",
    "    rms = librosa.feature.rms(y=y)[0]\n",
    "    feats['energy_mean'] = np.mean(rms)\n",
    "    feats['energy_std'] = np.std(rms)\n",
    "    feats['energy_range'] = np.max(rms) - np.min(rms)\n",
    "    \n",
    "    # 3. Zero Crossing Rate (speech activity)\n",
    "    zcr = librosa.feature.zero_crossing_rate(y)[0]\n",
    "    feats['zcr_mean'] = np.mean(zcr)\n",
    "    feats['zcr_std'] = np.std(zcr)\n",
    "    \n",
    "    # 4. Spectral Features (voice quality)\n",
    "    centroid = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "    feats['centroid_mean'] = np.mean(centroid)\n",
    "    feats['centroid_std'] = np.std(centroid)\n",
    "    \n",
    "    rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)[0]\n",
    "    feats['rolloff_mean'] = np.mean(rolloff)\n",
    "    feats['rolloff_std'] = np.std(rolloff)\n",
    "    \n",
    "    bandwidth = librosa.feature.spectral_bandwidth(y=y, sr=sr)[0]\n",
    "    feats['bandwidth_mean'] = np.mean(bandwidth)\n",
    "    \n",
    "    # 5. Pause Detection (depression = more pauses/silence)\n",
    "    intervals = librosa.effects.split(y, top_db=20)\n",
    "    total_duration = len(y) / sr\n",
    "    if len(intervals) > 0:\n",
    "        speech_duration = sum((end - start) / sr for start, end in intervals)\n",
    "        feats['silence_ratio'] = 1 - (speech_duration / total_duration)\n",
    "        feats['num_pauses'] = len(intervals) - 1\n",
    "    else:\n",
    "        feats['silence_ratio'] = 1.0\n",
    "        feats['num_pauses'] = 0\n",
    "    \n",
    "    # 6. Speech Rate (onset strength)\n",
    "    onset_env = librosa.onset.onset_strength(y=y, sr=sr)\n",
    "    feats['onset_mean'] = np.mean(onset_env)\n",
    "    feats['onset_std'] = np.std(onset_env)\n",
    "    \n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8bef883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average duration: 964.0s\n",
      "Min duration: 414.8s\n",
      "Max duration: 1966.2s\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import contextlib\n",
    "\n",
    "durations = []\n",
    "for pid in all_participant_ids:\n",
    "    file_path = f\"D:/depressiondetector/diag-woz/{pid}_P/{pid}_P/{pid}_AUDIO.wav\"\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(file_path, 'r')) as f:\n",
    "            frames = f.getnframes()\n",
    "            rate = f.getframerate()\n",
    "            duration = frames / float(rate)\n",
    "            durations.append(duration)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"Average duration: {np.mean(durations):.1f}s\")\n",
    "print(f\"Min duration: {np.min(durations):.1f}s\")\n",
    "print(f\"Max duration: {np.max(durations):.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "506db736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average duration: 973.1s\n",
      "Min duration: 414.8s\n",
      "Max duration: 1966.2s\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import contextlib\n",
    "\n",
    "durations = []\n",
    "for pid in train_ids:\n",
    "    file_path = f\"D:/depressiondetector/diag-woz/{pid}_P/{pid}_P/{pid}_AUDIO.wav\"\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(file_path, 'r')) as f:\n",
    "            frames = f.getnframes()\n",
    "            rate = f.getframerate()\n",
    "            duration = frames / float(rate)\n",
    "            durations.append(duration)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"Average duration: {np.mean(durations):.1f}s\")\n",
    "print(f\"Min duration: {np.min(durations):.1f}s\")\n",
    "print(f\"Max duration: {np.max(durations):.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "60db4a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average duration: 972.2s\n",
      "Min duration: 525.1s\n",
      "Max duration: 1686.8s\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import contextlib\n",
    "\n",
    "durations = []\n",
    "for pid in test_ids:\n",
    "    file_path = f\"D:/depressiondetector/diag-woz/{pid}_P/{pid}_P/{pid}_AUDIO.wav\"\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(file_path, 'r')) as f:\n",
    "            frames = f.getnframes()\n",
    "            rate = f.getframerate()\n",
    "            duration = frames / float(rate)\n",
    "            durations.append(duration)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"Average duration: {np.mean(durations):.1f}s\")\n",
    "print(f\"Min duration: {np.min(durations):.1f}s\")\n",
    "print(f\"Max duration: {np.max(durations):.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f797d46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average duration: 929.5s\n",
      "Min duration: 533.3s\n",
      "Max duration: 1420.0s\n"
     ]
    }
   ],
   "source": [
    "import wave\n",
    "import contextlib\n",
    "\n",
    "durations = []\n",
    "for pid in dev_ids:\n",
    "    file_path = f\"D:/depressiondetector/diag-woz/{pid}_P/{pid}_P/{pid}_AUDIO.wav\"\n",
    "    try:\n",
    "        with contextlib.closing(wave.open(file_path, 'r')) as f:\n",
    "            frames = f.getnframes()\n",
    "            rate = f.getframerate()\n",
    "            duration = frames / float(rate)\n",
    "            durations.append(duration)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"Average duration: {np.mean(durations):.1f}s\")\n",
    "print(f\"Min duration: {np.min(durations):.1f}s\")\n",
    "print(f\"Max duration: {np.max(durations):.1f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7f84a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting audio feature extraction for 275 participants...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03da4b3a8bf5449fafc5ee066467dc2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "participants:   0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 632: [Errno 2] No such file or directory: 'D:/depressiondetector/diag-woz/632_P/632_P/632_AUDIO.wav'\n",
      "Error processing 633: [Errno 2] No such file or directory: 'D:/depressiondetector/diag-woz/633_P/633_P/633_AUDIO.wav'\n",
      "Error processing 661: [Errno 2] No such file or directory: 'D:/depressiondetector/diag-woz/661_P/661_P/661_AUDIO.wav'\n",
      "Error processing 662: [Errno 2] No such file or directory: 'D:/depressiondetector/diag-woz/662_P/662_P/662_AUDIO.wav'\n",
      "Error processing 663: [Errno 2] No such file or directory: 'D:/depressiondetector/diag-woz/663_P/663_P/663_AUDIO.wav'\n",
      "Error processing 664: [Errno 2] No such file or directory: 'D:/depressiondetector/diag-woz/664_P/664_P/664_AUDIO.wav'\n",
      "Extraction finished. Extracted features for 269 participants.\n",
      "Missing files: 0 (first 10): []\n"
     ]
    }
   ],
   "source": [
    "all_features = []\n",
    "missing_files = []\n",
    "print(f\"Starting audio feature extraction for {len(all_participant_ids)} participants...\")\n",
    "\n",
    "for idx, pid in enumerate(tqdm(all_participant_ids, desc=\"participants\"), start=1):\n",
    "    pid_str = str(pid)\n",
    "    folder_name = f\"{pid_str}_P\"\n",
    "    file_name = f\"{pid_str}_AUDIO.wav\"\n",
    "    file_path = f\"D:/depressiondetector/diag-woz/{folder_name}/{folder_name}/{file_name}\"\n",
    "    try:\n",
    "        feats = extract_features_from_file(file_path, sr_target=16000, duration=180)  # load up to 60s\n",
    "        feats['Participant_ID'] = pid_str\n",
    "        all_features.append(feats)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {pid_str}: {e}\")\n",
    "\n",
    "print(f\"Extraction finished. Extracted features for {len(all_features)} participants.\")\n",
    "print(f\"Missing files: {len(missing_files)} (first 10):\", missing_files[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bdf27727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'mfcc_mean': np.float32(-14.795773),\n",
       "  'mfcc_std': np.float32(99.142136),\n",
       "  'mfcc_min': np.float32(-533.9602),\n",
       "  'mfcc_max': np.float32(177.34402),\n",
       "  'chroma_mean': np.float32(0.3885395),\n",
       "  'chroma_std': np.float32(0.30154717),\n",
       "  'chroma_min': np.float32(0.00084337924),\n",
       "  'chroma_max': np.float32(1.0),\n",
       "  'mel_mean': np.float32(0.038381375),\n",
       "  'mel_std': np.float32(1.8338321),\n",
       "  'mel_min': np.float32(8.256466e-08),\n",
       "  'mel_max': np.float32(544.17773),\n",
       "  'contrast_mean': np.float64(16.10802707595517),\n",
       "  'contrast_std': np.float64(3.449186360614551),\n",
       "  'contrast_min': np.float64(5.814903246286432),\n",
       "  'contrast_max': np.float64(45.859467778186776),\n",
       "  'tonnetz_mean': np.float64(-0.02642035126228308),\n",
       "  'tonnetz_std': np.float64(0.13178216765735398),\n",
       "  'tonnetz_min': np.float64(-0.6497567244805398),\n",
       "  'tonnetz_max': np.float64(0.675819401629269),\n",
       "  'pitch_mean': np.float64(246.13021961324466),\n",
       "  'pitch_std': np.float64(263.6960673538121),\n",
       "  'pitch_range': np.float64(2139.7633136094673),\n",
       "  'energy_mean': np.float32(0.0056305206),\n",
       "  'energy_std': np.float32(0.011682659),\n",
       "  'energy_range': np.float32(0.17085965),\n",
       "  'zcr_mean': np.float64(0.09421007445898506),\n",
       "  'zcr_std': np.float64(0.03989251921260489),\n",
       "  'centroid_mean': np.float64(1943.9373346787274),\n",
       "  'centroid_std': np.float64(468.30598021562827),\n",
       "  'rolloff_mean': np.float64(4513.490657216495),\n",
       "  'rolloff_std': np.float64(1082.8379488099063),\n",
       "  'bandwidth_mean': np.float64(2085.591297485222),\n",
       "  'silence_ratio': np.float64(0.9548444444444444),\n",
       "  'num_pauses': 31,\n",
       "  'onset_mean': np.float32(0.9757182),\n",
       "  'onset_std': np.float32(0.77112633),\n",
       "  'Participant_ID': '300'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_features[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d7b4b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved features to: D:\\depressiondetector\\extracted_features\\audio_features.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>mfcc_mean</th>\n",
       "      <th>mfcc_std</th>\n",
       "      <th>mfcc_min</th>\n",
       "      <th>mfcc_max</th>\n",
       "      <th>chroma_mean</th>\n",
       "      <th>chroma_std</th>\n",
       "      <th>chroma_min</th>\n",
       "      <th>chroma_max</th>\n",
       "      <th>mel_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>zcr_std</th>\n",
       "      <th>centroid_mean</th>\n",
       "      <th>centroid_std</th>\n",
       "      <th>rolloff_mean</th>\n",
       "      <th>rolloff_std</th>\n",
       "      <th>bandwidth_mean</th>\n",
       "      <th>silence_ratio</th>\n",
       "      <th>num_pauses</th>\n",
       "      <th>onset_mean</th>\n",
       "      <th>onset_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>-14.795773</td>\n",
       "      <td>99.142136</td>\n",
       "      <td>-533.960205</td>\n",
       "      <td>177.344025</td>\n",
       "      <td>0.388539</td>\n",
       "      <td>0.301547</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.038381</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039893</td>\n",
       "      <td>1943.937335</td>\n",
       "      <td>468.305980</td>\n",
       "      <td>4513.490657</td>\n",
       "      <td>1082.837949</td>\n",
       "      <td>2085.591297</td>\n",
       "      <td>0.954844</td>\n",
       "      <td>31</td>\n",
       "      <td>0.975718</td>\n",
       "      <td>0.771126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>-14.631455</td>\n",
       "      <td>104.950478</td>\n",
       "      <td>-543.891418</td>\n",
       "      <td>225.267242</td>\n",
       "      <td>0.465250</td>\n",
       "      <td>0.308267</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.032293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.055971</td>\n",
       "      <td>1531.415721</td>\n",
       "      <td>574.182551</td>\n",
       "      <td>3560.869734</td>\n",
       "      <td>1378.155150</td>\n",
       "      <td>1821.564730</td>\n",
       "      <td>0.757511</td>\n",
       "      <td>110</td>\n",
       "      <td>1.258738</td>\n",
       "      <td>1.220680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>-14.796026</td>\n",
       "      <td>107.798409</td>\n",
       "      <td>-576.355042</td>\n",
       "      <td>180.074036</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.301694</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041391</td>\n",
       "      <td>1596.970103</td>\n",
       "      <td>462.771811</td>\n",
       "      <td>3764.932068</td>\n",
       "      <td>1058.536113</td>\n",
       "      <td>1903.019419</td>\n",
       "      <td>0.691378</td>\n",
       "      <td>133</td>\n",
       "      <td>1.149674</td>\n",
       "      <td>0.993042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>-16.326721</td>\n",
       "      <td>106.826447</td>\n",
       "      <td>-562.320007</td>\n",
       "      <td>206.378250</td>\n",
       "      <td>0.398873</td>\n",
       "      <td>0.319536</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>...</td>\n",
       "      <td>0.039343</td>\n",
       "      <td>1407.789278</td>\n",
       "      <td>490.247017</td>\n",
       "      <td>3176.833841</td>\n",
       "      <td>1294.371725</td>\n",
       "      <td>1727.703705</td>\n",
       "      <td>0.695467</td>\n",
       "      <td>138</td>\n",
       "      <td>1.365498</td>\n",
       "      <td>1.325724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>-16.780870</td>\n",
       "      <td>113.715179</td>\n",
       "      <td>-562.917175</td>\n",
       "      <td>203.109924</td>\n",
       "      <td>0.431276</td>\n",
       "      <td>0.318336</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024001</td>\n",
       "      <td>1332.510559</td>\n",
       "      <td>302.009992</td>\n",
       "      <td>3070.330552</td>\n",
       "      <td>956.134064</td>\n",
       "      <td>1722.597308</td>\n",
       "      <td>0.765156</td>\n",
       "      <td>119</td>\n",
       "      <td>1.146712</td>\n",
       "      <td>1.024693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant_ID  mfcc_mean    mfcc_std    mfcc_min    mfcc_max  chroma_mean  \\\n",
       "0            300 -14.795773   99.142136 -533.960205  177.344025     0.388539   \n",
       "1            301 -14.631455  104.950478 -543.891418  225.267242     0.465250   \n",
       "2            302 -14.796026  107.798409 -576.355042  180.074036     0.492308   \n",
       "3            303 -16.326721  106.826447 -562.320007  206.378250     0.398873   \n",
       "4            304 -16.780870  113.715179 -562.917175  203.109924     0.431276   \n",
       "\n",
       "   chroma_std  chroma_min  chroma_max  mel_mean  ...   zcr_std  centroid_mean  \\\n",
       "0    0.301547    0.000843         1.0  0.038381  ...  0.039893    1943.937335   \n",
       "1    0.308267    0.000861         1.0  0.032293  ...  0.055971    1531.415721   \n",
       "2    0.301694    0.001593         1.0  0.007299  ...  0.041391    1596.970103   \n",
       "3    0.319536    0.000290         1.0  0.022962  ...  0.039343    1407.789278   \n",
       "4    0.318336    0.000286         1.0  0.007488  ...  0.024001    1332.510559   \n",
       "\n",
       "   centroid_std  rolloff_mean  rolloff_std  bandwidth_mean  silence_ratio  \\\n",
       "0    468.305980   4513.490657  1082.837949     2085.591297       0.954844   \n",
       "1    574.182551   3560.869734  1378.155150     1821.564730       0.757511   \n",
       "2    462.771811   3764.932068  1058.536113     1903.019419       0.691378   \n",
       "3    490.247017   3176.833841  1294.371725     1727.703705       0.695467   \n",
       "4    302.009992   3070.330552   956.134064     1722.597308       0.765156   \n",
       "\n",
       "   num_pauses  onset_mean  onset_std  \n",
       "0          31    0.975718   0.771126  \n",
       "1         110    1.258738   1.220680  \n",
       "2         133    1.149674   0.993042  \n",
       "3         138    1.365498   1.325724  \n",
       "4         119    1.146712   1.024693  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = pd.DataFrame(all_features)\n",
    "# Sort columns to put Participant_ID first\n",
    "cols = ['Participant_ID'] + [c for c in df_features.columns if c != 'Participant_ID']\n",
    "df_features = df_features[cols]\n",
    "df_features.to_csv(OUTPUT_CSV_FILE, index=False)\n",
    "print(\"Saved features to:\", OUTPUT_CSV_FILE)\n",
    "df_features.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4776476c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (161, 43)\n",
      "dev_df:   (55, 43)\n",
      "test_df:  (53, 43)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>mfcc_mean</th>\n",
       "      <th>mfcc_std</th>\n",
       "      <th>mfcc_min</th>\n",
       "      <th>mfcc_max</th>\n",
       "      <th>chroma_mean</th>\n",
       "      <th>chroma_std</th>\n",
       "      <th>chroma_min</th>\n",
       "      <th>chroma_max</th>\n",
       "      <th>mel_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>bandwidth_mean</th>\n",
       "      <th>silence_ratio</th>\n",
       "      <th>num_pauses</th>\n",
       "      <th>onset_mean</th>\n",
       "      <th>onset_std</th>\n",
       "      <th>Gender</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>PHQ_Score</th>\n",
       "      <th>PCL-C (PTSD)</th>\n",
       "      <th>PTSD Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>302</td>\n",
       "      <td>-14.796026</td>\n",
       "      <td>107.798410</td>\n",
       "      <td>-576.35504</td>\n",
       "      <td>180.07404</td>\n",
       "      <td>0.492308</td>\n",
       "      <td>0.301694</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>...</td>\n",
       "      <td>1903.019419</td>\n",
       "      <td>0.691378</td>\n",
       "      <td>133</td>\n",
       "      <td>1.149674</td>\n",
       "      <td>0.993042</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>303</td>\n",
       "      <td>-16.326721</td>\n",
       "      <td>106.826450</td>\n",
       "      <td>-562.32000</td>\n",
       "      <td>206.37825</td>\n",
       "      <td>0.398873</td>\n",
       "      <td>0.319536</td>\n",
       "      <td>0.000290</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.022962</td>\n",
       "      <td>...</td>\n",
       "      <td>1727.703705</td>\n",
       "      <td>0.695467</td>\n",
       "      <td>138</td>\n",
       "      <td>1.365498</td>\n",
       "      <td>1.325724</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>304</td>\n",
       "      <td>-16.780870</td>\n",
       "      <td>113.715180</td>\n",
       "      <td>-562.91720</td>\n",
       "      <td>203.10992</td>\n",
       "      <td>0.431276</td>\n",
       "      <td>0.318336</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007488</td>\n",
       "      <td>...</td>\n",
       "      <td>1722.597308</td>\n",
       "      <td>0.765156</td>\n",
       "      <td>119</td>\n",
       "      <td>1.146712</td>\n",
       "      <td>1.024693</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>305</td>\n",
       "      <td>-6.683355</td>\n",
       "      <td>74.546936</td>\n",
       "      <td>-436.89380</td>\n",
       "      <td>241.80798</td>\n",
       "      <td>0.372353</td>\n",
       "      <td>0.307797</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.248431</td>\n",
       "      <td>...</td>\n",
       "      <td>1082.563587</td>\n",
       "      <td>0.498133</td>\n",
       "      <td>164</td>\n",
       "      <td>1.133962</td>\n",
       "      <td>1.145486</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>307</td>\n",
       "      <td>-6.802663</td>\n",
       "      <td>69.846980</td>\n",
       "      <td>-414.14072</td>\n",
       "      <td>213.75266</td>\n",
       "      <td>0.329199</td>\n",
       "      <td>0.317432</td>\n",
       "      <td>0.000107</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.884708</td>\n",
       "      <td>...</td>\n",
       "      <td>1171.305341</td>\n",
       "      <td>0.495822</td>\n",
       "      <td>195</td>\n",
       "      <td>1.315440</td>\n",
       "      <td>1.406614</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant_ID  mfcc_mean    mfcc_std   mfcc_min   mfcc_max  chroma_mean  \\\n",
       "0            302 -14.796026  107.798410 -576.35504  180.07404     0.492308   \n",
       "1            303 -16.326721  106.826450 -562.32000  206.37825     0.398873   \n",
       "2            304 -16.780870  113.715180 -562.91720  203.10992     0.431276   \n",
       "3            305  -6.683355   74.546936 -436.89380  241.80798     0.372353   \n",
       "4            307  -6.802663   69.846980 -414.14072  213.75266     0.329199   \n",
       "\n",
       "   chroma_std  chroma_min  chroma_max  mel_mean  ...  bandwidth_mean  \\\n",
       "0    0.301694    0.001593         1.0  0.007299  ...     1903.019419   \n",
       "1    0.319536    0.000290         1.0  0.022962  ...     1727.703705   \n",
       "2    0.318336    0.000286         1.0  0.007488  ...     1722.597308   \n",
       "3    0.307797    0.000101         1.0  1.248431  ...     1082.563587   \n",
       "4    0.317432    0.000107         1.0  1.884708  ...     1171.305341   \n",
       "\n",
       "   silence_ratio  num_pauses  onset_mean  onset_std   Gender  PHQ8_Binary  \\\n",
       "0       0.691378         133    1.149674   0.993042     male            0   \n",
       "1       0.695467         138    1.365498   1.325724   female            0   \n",
       "2       0.765156         119    1.146712   1.024693   female            0   \n",
       "3       0.498133         164    1.133962   1.145486     male            0   \n",
       "4       0.495822         195    1.315440   1.406614  female             0   \n",
       "\n",
       "   PHQ_Score  PCL-C (PTSD)  PTSD Severity  \n",
       "0          4             0             28  \n",
       "1          0             0             17  \n",
       "2          6             0             20  \n",
       "3          7             0             28  \n",
       "4          4             0             23  \n",
       "\n",
       "[5 rows x 43 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload features (safe)\n",
    "features_df = pd.read_csv(OUTPUT_CSV_FILE).astype({\"Participant_ID\": str})\n",
    "\n",
    "# Ensure label dataframes Participant_ID are str\n",
    "train_labels['Participant_ID'] = train_labels['Participant_ID'].astype(str)\n",
    "dev_labels['Participant_ID'] = dev_labels['Participant_ID'].astype(str)\n",
    "test_labels['Participant_ID'] = test_labels['Participant_ID'].astype(str)\n",
    "\n",
    "# Merge\n",
    "train_df = pd.merge(features_df, train_labels, on='Participant_ID', how='inner')\n",
    "dev_df   = pd.merge(features_df, dev_labels, on='Participant_ID', how='inner')\n",
    "test_df  = pd.merge(features_df, test_labels, on='Participant_ID', how='inner')\n",
    "\n",
    "print(\"train_df:\", train_df.shape)\n",
    "print(\"dev_df:  \", dev_df.shape)\n",
    "print(\"test_df: \", test_df.shape)\n",
    "\n",
    "# Quick check: show a couple rows\n",
    "train_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6a0372a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df: (161, 26)\n",
      "dev_df:   (55, 26)\n",
      "test_df:  (53, 26)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>mfcc_mean</th>\n",
       "      <th>mfcc_std</th>\n",
       "      <th>mfcc_min</th>\n",
       "      <th>mfcc_max</th>\n",
       "      <th>chroma_mean</th>\n",
       "      <th>chroma_std</th>\n",
       "      <th>chroma_min</th>\n",
       "      <th>chroma_max</th>\n",
       "      <th>mel_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>contrast_max</th>\n",
       "      <th>tonnetz_mean</th>\n",
       "      <th>tonnetz_std</th>\n",
       "      <th>tonnetz_min</th>\n",
       "      <th>tonnetz_max</th>\n",
       "      <th>Gender</th>\n",
       "      <th>PHQ8_Binary</th>\n",
       "      <th>PHQ_Score</th>\n",
       "      <th>PCL-C (PTSD)</th>\n",
       "      <th>PTSD Severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>302</td>\n",
       "      <td>-22.704115</td>\n",
       "      <td>125.54751</td>\n",
       "      <td>-576.35504</td>\n",
       "      <td>159.41074</td>\n",
       "      <td>0.500923</td>\n",
       "      <td>0.309544</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012573</td>\n",
       "      <td>...</td>\n",
       "      <td>37.810635</td>\n",
       "      <td>-0.012819</td>\n",
       "      <td>0.069096</td>\n",
       "      <td>-0.369869</td>\n",
       "      <td>0.344158</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>303</td>\n",
       "      <td>-25.683535</td>\n",
       "      <td>137.78560</td>\n",
       "      <td>-574.64610</td>\n",
       "      <td>186.30038</td>\n",
       "      <td>0.414608</td>\n",
       "      <td>0.315139</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.012661</td>\n",
       "      <td>...</td>\n",
       "      <td>39.142904</td>\n",
       "      <td>-0.014607</td>\n",
       "      <td>0.085247</td>\n",
       "      <td>-0.516046</td>\n",
       "      <td>0.476484</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>304</td>\n",
       "      <td>-26.678469</td>\n",
       "      <td>139.74298</td>\n",
       "      <td>-572.50555</td>\n",
       "      <td>166.64377</td>\n",
       "      <td>0.427352</td>\n",
       "      <td>0.320371</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.006618</td>\n",
       "      <td>...</td>\n",
       "      <td>44.504158</td>\n",
       "      <td>-0.021001</td>\n",
       "      <td>0.089289</td>\n",
       "      <td>-0.540239</td>\n",
       "      <td>0.500995</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>305</td>\n",
       "      <td>-9.534797</td>\n",
       "      <td>89.30562</td>\n",
       "      <td>-436.89380</td>\n",
       "      <td>220.43200</td>\n",
       "      <td>0.334624</td>\n",
       "      <td>0.319431</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.701195</td>\n",
       "      <td>...</td>\n",
       "      <td>43.080801</td>\n",
       "      <td>0.004412</td>\n",
       "      <td>0.118239</td>\n",
       "      <td>-0.635300</td>\n",
       "      <td>0.658467</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>307</td>\n",
       "      <td>-9.506833</td>\n",
       "      <td>89.39855</td>\n",
       "      <td>-430.17856</td>\n",
       "      <td>215.82660</td>\n",
       "      <td>0.328442</td>\n",
       "      <td>0.318607</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.689439</td>\n",
       "      <td>...</td>\n",
       "      <td>41.626073</td>\n",
       "      <td>-0.010205</td>\n",
       "      <td>0.123659</td>\n",
       "      <td>-0.662332</td>\n",
       "      <td>0.658560</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant_ID  mfcc_mean   mfcc_std   mfcc_min   mfcc_max  chroma_mean  \\\n",
       "0            302 -22.704115  125.54751 -576.35504  159.41074     0.500923   \n",
       "1            303 -25.683535  137.78560 -574.64610  186.30038     0.414608   \n",
       "2            304 -26.678469  139.74298 -572.50555  166.64377     0.427352   \n",
       "3            305  -9.534797   89.30562 -436.89380  220.43200     0.334624   \n",
       "4            307  -9.506833   89.39855 -430.17856  215.82660     0.328442   \n",
       "\n",
       "   chroma_std  chroma_min  chroma_max  mel_mean  ...  contrast_max  \\\n",
       "0    0.309544    0.001593         1.0  0.012573  ...     37.810635   \n",
       "1    0.315139    0.001407         1.0  0.012661  ...     39.142904   \n",
       "2    0.320371    0.000836         1.0  0.006618  ...     44.504158   \n",
       "3    0.319431    0.000096         1.0  1.701195  ...     43.080801   \n",
       "4    0.318607    0.000379         1.0  1.689439  ...     41.626073   \n",
       "\n",
       "   tonnetz_mean  tonnetz_std  tonnetz_min  tonnetz_max   Gender  PHQ8_Binary  \\\n",
       "0     -0.012819     0.069096    -0.369869     0.344158     male            0   \n",
       "1     -0.014607     0.085247    -0.516046     0.476484   female            0   \n",
       "2     -0.021001     0.089289    -0.540239     0.500995   female            0   \n",
       "3      0.004412     0.118239    -0.635300     0.658467     male            0   \n",
       "4     -0.010205     0.123659    -0.662332     0.658560  female             0   \n",
       "\n",
       "   PHQ_Score  PCL-C (PTSD)  PTSD Severity  \n",
       "0          4             0             28  \n",
       "1          0             0             17  \n",
       "2          6             0             20  \n",
       "3          7             0             28  \n",
       "4          4             0             23  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload features (safe)\n",
    "features_df = pd.read_csv(OUTPUT_CSV_FILE).astype({\"Participant_ID\": str})\n",
    "\n",
    "# Ensure label dataframes Participant_ID are str\n",
    "train_labels['Participant_ID'] = train_labels['Participant_ID'].astype(str)\n",
    "dev_labels['Participant_ID'] = dev_labels['Participant_ID'].astype(str)\n",
    "test_labels['Participant_ID'] = test_labels['Participant_ID'].astype(str)\n",
    "\n",
    "# Merge\n",
    "train_df = pd.merge(features_df, train_labels, on='Participant_ID', how='inner')\n",
    "dev_df   = pd.merge(features_df, dev_labels, on='Participant_ID', how='inner')\n",
    "test_df  = pd.merge(features_df, test_labels, on='Participant_ID', how='inner')\n",
    "\n",
    "print(\"train_df:\", train_df.shape)\n",
    "print(\"dev_df:  \", dev_df.shape)\n",
    "print(\"test_df: \", test_df.shape)\n",
    "\n",
    "# Quick check: show a couple rows\n",
    "train_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a32f49a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify feature columns (everything in features_df except Participant_ID)\n",
    "feature_cols = [c for c in features_df.columns if c != 'Participant_ID']\n",
    "\n",
    "# Ensure the target column exists in merged dfs\n",
    "if 'PHQ8_Binary' not in train_df.columns:\n",
    "    raise KeyError(\"PHQ8_Binary not found in merged train_df. Check label normalization step.\")\n",
    "\n",
    "# Build X, y\n",
    "X_train = train_df[feature_cols].copy()\n",
    "y_train = train_df['PHQ8_Binary'].astype(int).copy()\n",
    "\n",
    "X_dev = dev_df[feature_cols].copy()\n",
    "y_dev = dev_df['PHQ8_Binary'].astype(int).copy()\n",
    "\n",
    "X_test = test_df[feature_cols].copy()\n",
    "y_test = test_df['PHQ8_Binary'].astype(int).copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c352cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\n",
      "X shapes: (161, 37)\n",
      "y distribution (train):\n",
      " PHQ8_Binary\n",
      "0    124\n",
      "1     37\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\")\n",
    "print(\"X shapes:\", X_train.shape)\n",
    "print(\"y distribution (train):\\n\", y_train.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7a1ea002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n",
      "X shapes: (53, 37)\n",
      "y distribution (train):\n",
      " PHQ8_Binary\n",
      "0    37\n",
      "1    16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data:\")\n",
    "print(\"X shapes:\", X_test.shape)\n",
    "print(\"y distribution (train):\\n\", y_test.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "adef7336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data:\n",
      "X shapes: (55, 37)\n",
      "y distribution (train):\n",
      " PHQ8_Binary\n",
      "0    43\n",
      "1    12\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Test data:\")\n",
    "print(\"X shapes:\", X_dev.shape)\n",
    "print(\"y distribution (train):\\n\", y_dev.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "662132ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_dev_scaled   = scaler.transform(X_dev)\n",
    "X_test_scaled  = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "02064343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Logistic Regression on Dev set ---\n",
      "Accuracy: 0.5454545454545454\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed (0)       0.76      0.60      0.68        43\n",
      "    Depressed (1)       0.19      0.33      0.24        12\n",
      "\n",
      "         accuracy                           0.55        55\n",
      "        macro avg       0.48      0.47      0.46        55\n",
      "     weighted avg       0.64      0.55      0.58        55\n",
      "\n",
      "\n",
      "--- Logistic Regression on Test set ---\n",
      "Accuracy: 0.4528301886792453\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed (0)       0.63      0.51      0.57        37\n",
      "    Depressed (1)       0.22      0.31      0.26        16\n",
      "\n",
      "         accuracy                           0.45        53\n",
      "        macro avg       0.43      0.41      0.41        53\n",
      "     weighted avg       0.51      0.45      0.47        53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Logistic regression baseline\n",
    "lr = LogisticRegression(random_state=42, class_weight='balanced', max_iter=1000)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_dev_pred = lr.predict(X_dev_scaled)\n",
    "print(\"--- Logistic Regression on Dev set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_dev, y_dev_pred))\n",
    "print(classification_report(y_dev, y_dev_pred, target_names=['Not Depressed (0)','Depressed (1)']))\n",
    "\n",
    "y_test_pred = lr.predict(X_test_scaled)\n",
    "print(\"\\n--- Logistic Regression on Test set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Not Depressed (0)','Depressed (1)']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d4a612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest on Dev set ---\n",
      "Accuracy: 0.7636363636363637\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed (0)       0.78      0.98      0.87        43\n",
      "    Depressed (1)       0.00      0.00      0.00        12\n",
      "\n",
      "         accuracy                           0.76        55\n",
      "        macro avg       0.39      0.49      0.43        55\n",
      "     weighted avg       0.61      0.76      0.68        55\n",
      "\n",
      "\n",
      "--- Random Forest on Test set ---\n",
      "Accuracy: 0.660377358490566\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed (0)       0.69      0.95      0.80        37\n",
      "    Depressed (1)       0.00      0.00      0.00        16\n",
      "\n",
      "         accuracy                           0.66        53\n",
      "        macro avg       0.34      0.47      0.40        53\n",
      "     weighted avg       0.48      0.66      0.56        53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest baseline\n",
    "rf = RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=100)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_dev_rf = rf.predict(X_dev_scaled)\n",
    "print(\"\\n--- Random Forest on Dev set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_dev, y_dev_rf))\n",
    "print(classification_report(y_dev, y_dev_rf, target_names=['Not Depressed (0)','Depressed (1)']))\n",
    "\n",
    "\n",
    "y_test_rf = rf.predict(X_test_scaled)\n",
    "print(\"\\n--- Random Forest on Test set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_rf))\n",
    "print(classification_report(y_test, y_test_rf, target_names=['Not Depressed (0)','Depressed (1)']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85340a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost on Test set ---\n",
      "Accuracy: 0.49056603773584906\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed (0)       0.62      0.68      0.65        37\n",
      "    Depressed (1)       0.08      0.06      0.07        16\n",
      "\n",
      "         accuracy                           0.49        53\n",
      "        macro avg       0.35      0.37      0.36        53\n",
      "     weighted avg       0.46      0.49      0.47        53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Add after your Random Forest code\n",
    "xgb = XGBClassifier(\n",
    "    random_state=42,\n",
    "    scale_pos_weight=len(y_train[y_train==0]) / len(y_train[y_train==1]),  # Handle imbalance\n",
    "    n_estimators=200,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8\n",
    ")\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_xgb = xgb.predict(X_test_scaled)\n",
    "print(\"\\n--- XGBoost on Test set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_xgb))\n",
    "print(classification_report(y_test, y_test_xgb, target_names=['Not Depressed (0)','Depressed (1)']))\n",
    "\n",
    "\n",
    "y_test_xgb = xgb.predict(X_test_scaled)\n",
    "print(\"\\n--- XGBoost on Test set ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_xgb))\n",
    "print(classification_report(y_test, y_test_xgb, target_names=['Not Depressed (0)','Depressed (1)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "472138ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Random Forest with Feature Selection  on test---\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed (0)       0.71      0.97      0.82        37\n",
      "    Depressed (1)       0.50      0.06      0.11        16\n",
      "\n",
      "         accuracy                           0.70        53\n",
      "        macro avg       0.60      0.52      0.46        53\n",
      "     weighted avg       0.64      0.70      0.60        53\n",
      "\n",
      "\n",
      "--- Random Forest with Feature Selection on train  ---\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed (0)       0.76      0.91      0.83        43\n",
      "    Depressed (1)       0.00      0.00      0.00        12\n",
      "\n",
      "         accuracy                           0.71        55\n",
      "        macro avg       0.38      0.45      0.41        55\n",
      "     weighted avg       0.60      0.71      0.65        55\n",
      "\n",
      "\n",
      "Selected features: ['mfcc_min', 'chroma_mean', 'mel_mean', 'mel_min', 'mel_max', 'contrast_mean', 'contrast_std', 'contrast_min', 'contrast_max', 'tonnetz_mean', 'pitch_mean', 'pitch_std', 'pitch_range', 'energy_range', 'zcr_mean', 'zcr_std', 'centroid_mean', 'centroid_std', 'rolloff_mean', 'rolloff_std', 'bandwidth_mean', 'silence_ratio', 'num_pauses', 'onset_mean', 'onset_std']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# Select top 25 features\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "X_dev_selected = selector.transform(X_dev_scaled)\n",
    "\n",
    "# Train on selected features\n",
    "rf_selected = RandomForestClassifier(random_state=42, class_weight='balanced', n_estimators=200)\n",
    "rf_selected.fit(X_train_selected, y_train)\n",
    "\n",
    "y_test_selected = rf_selected.predict(X_test_selected)\n",
    "print(\"\\n--- Random Forest with Feature Selection  on test---\")\n",
    "print(classification_report(y_test, y_test_selected, target_names=['Not Depressed (0)','Depressed (1)']))\n",
    "\n",
    "y_dev_selected = rf_selected.predict(X_dev_selected)\n",
    "print(\"\\n--- Random Forest with Feature Selection on train  ---\")\n",
    "print(classification_report(y_dev, y_dev_selected, target_names=['Not Depressed (0)','Depressed (1)']))\n",
    "\n",
    "\n",
    "# Show which features were selected\n",
    "selected_features = [feature_cols[i] for i in selector.get_support(indices=True)]\n",
    "print(\"\\nSelected features:\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aea49424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: {0: 124, 1: 37}\n",
      "After SMOTE:  {0: 124, 1: 124}\n",
      "\n",
      "--- Random Forest (SMOTE) on Test set ---\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed (0)       0.69      0.78      0.73        37\n",
      "    Depressed (1)       0.27      0.19      0.22        16\n",
      "\n",
      "         accuracy                           0.60        53\n",
      "        macro avg       0.48      0.49      0.48        53\n",
      "     weighted avg       0.56      0.60      0.58        53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# SMOTE on numerical features\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_res, y_train_res = smote.fit_resample(X_train, y_train)\n",
    "print(\"Before SMOTE:\", y_train.value_counts().to_dict())\n",
    "print(\"After SMOTE: \", pd.Series(y_train_res).value_counts().to_dict())\n",
    "\n",
    "# Scale again (fit on resampled training set)\n",
    "scaler_smote = StandardScaler()\n",
    "X_train_res_scaled = scaler_smote.fit_transform(X_train_res)\n",
    "X_test_scaled_smote = scaler_smote.transform(X_test)  # transform test with same scaler\n",
    "\n",
    "rf_smote = RandomForestClassifier(random_state=42, n_estimators=200)\n",
    "rf_smote.fit(X_train_res_scaled, y_train_res)\n",
    "\n",
    "y_test_smote = rf_smote.predict(X_test_scaled_smote)\n",
    "print(\"\\n--- Random Forest (SMOTE) on Test set ---\")\n",
    "print(classification_report(y_test, y_test_smote, target_names=['Not Depressed (0)','Depressed (1)']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d4619ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original features: 37\n",
      "Selected features: 25\n",
      "\n",
      "Selected features: ['mfcc_min', 'chroma_mean', 'mel_mean', 'mel_min', 'mel_max', 'contrast_mean', 'contrast_std', 'contrast_min', 'contrast_max', 'tonnetz_mean', 'pitch_mean', 'pitch_std', 'pitch_range', 'energy_range', 'zcr_mean', 'zcr_std', 'centroid_mean', 'centroid_std', 'rolloff_mean', 'rolloff_std', 'bandwidth_mean', 'silence_ratio', 'num_pauses', 'onset_mean', 'onset_std']\n",
      "\n",
      "Before SMOTE: {0: 124, 1: 37}\n",
      "After SMOTE:  {0: 124, 1: 124}\n",
      "\n",
      "============================================================\n",
      "=== IMPROVED MODEL: Feature Selection + SMOTE + RF ===\n",
      "============================================================\n",
      "\n",
      "--- Test Set Results ---\n",
      "Accuracy: 0.5849056603773585\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed (0)       0.69      0.73      0.71        37\n",
      "    Depressed (1)       0.29      0.25      0.27        16\n",
      "\n",
      "         accuracy                           0.58        53\n",
      "        macro avg       0.49      0.49      0.49        53\n",
      "     weighted avg       0.57      0.58      0.58        53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ===== STEP 1: Scale Original Data =====\n",
    "scaler_initial = StandardScaler()\n",
    "X_train_scaled = scaler_initial.fit_transform(X_train)\n",
    "X_test_scaled = scaler_initial.transform(X_test)\n",
    "X_dev_scaled = scaler_initial.transform(X_dev)\n",
    "\n",
    "# ===== STEP 2: Feature Selection (on scaled data) =====\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "X_dev_selected = selector.transform(X_dev_scaled)\n",
    "\n",
    "print(f\"Original features: {X_train.shape[1]}\")\n",
    "print(f\"Selected features: {X_train_selected.shape[1]}\")\n",
    "\n",
    "# Show which features were selected\n",
    "selected_features = [feature_cols[i] for i in selector.get_support(indices=True)]\n",
    "print(\"\\nSelected features:\", selected_features)\n",
    "\n",
    "# ===== STEP 3: SMOTE on Selected Features =====\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_selected, y_train)\n",
    "print(f\"\\nBefore SMOTE: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"After SMOTE:  {pd.Series(y_train_smote).value_counts().to_dict()}\")\n",
    "\n",
    "# ===== STEP 4: Scale SMOTE Data =====\n",
    "scaler_smote = StandardScaler()\n",
    "X_train_smote_scaled = scaler_smote.fit_transform(X_train_smote)\n",
    "X_test_final = scaler_smote.transform(X_test_selected)\n",
    "X_dev_final = scaler_smote.transform(X_dev_selected)\n",
    "\n",
    "# ===== STEP 5: Train Random Forest =====\n",
    "rf_best = RandomForestClassifier(\n",
    "    random_state=42,\n",
    "    n_estimators=300,  # Increased from 200\n",
    "    max_depth=20,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    class_weight='balanced'  # Still use this for extra protection\n",
    ")\n",
    "\n",
    "rf_best.fit(X_train_smote_scaled, y_train_smote)\n",
    "\n",
    "# ===== STEP 6: Evaluate =====\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"=== IMPROVED MODEL: Feature Selection + SMOTE + RF ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dev set\n",
    "\n",
    "# Test set\n",
    "y_test_pred = rf_best.predict(X_test_final)\n",
    "print(\"\\n--- Test Set Results ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred, target_names=['Not Depressed (0)','Depressed (1)']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f1f5367f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models and scaler saved to D:\\depressiondetector\\extracted_features\\models\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "model_dir = os.path.join(OUTPUT_DIR, \"models\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "joblib.dump(lr, os.path.join(model_dir, \"logreg_baseline.joblib\"))\n",
    "joblib.dump(rf, os.path.join(model_dir, \"rf_baseline.joblib\"))\n",
    "joblib.dump(rf_smote, os.path.join(model_dir, \"rf_smote.joblib\"))\n",
    "joblib.dump(scaler_smote, os.path.join(model_dir, \"scaler_smote.joblib\"))\n",
    "print(\"Models and scaler saved to\", model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d336a846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "# ----------- Load Model & Scaler -----------\n",
    "model = joblib.load(\"D:/depressiondetector/extracted_features/models/rf_smote.joblib\")  # Random Forest trained on SMOTE data\n",
    "scaler = joblib.load(\"D:/depressiondetector/extracted_features/models/scaler_smote.joblib\")\n",
    "\n",
    "# ----------- Feature Extraction -----------\n",
    "def extract_features(file_path):\n",
    "    y, sr = librosa.load(file_path, sr=16000, duration=60)\n",
    "    y, _ = librosa.effects.trim(y)\n",
    "    feats = []\n",
    "    # MFCC (13)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    feats.extend([np.mean(mfccs), np.std(mfccs), np.min(mfccs), np.max(mfccs)])\n",
    "    # Chroma\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    feats.extend([np.mean(chroma), np.std(chroma), np.min(chroma), np.max(chroma)])\n",
    "    # Mel spectrogram\n",
    "    mel = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    feats.extend([np.mean(mel), np.std(mel), np.min(mel), np.max(mel)])\n",
    "    # Spectral contrast\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    feats.extend([np.mean(contrast), np.std(contrast), np.min(contrast), np.max(contrast)])\n",
    "    # Tonnetz\n",
    "    tonnetz = librosa.feature.tonnetz(y=librosa.effects.harmonic(y), sr=sr)\n",
    "    feats.extend([np.mean(tonnetz), np.std(tonnetz), np.min(tonnetz), np.max(tonnetz)])\n",
    "    return np.array(feats).reshape(1, -1)\n",
    "\n",
    "# ----------- Predict Function -----------\n",
    "def predict_audio(file_path):\n",
    "    features = extract_features(file_path)\n",
    "    features_scaled = scaler.transform(features)\n",
    "\n",
    "    prediction = model.predict(features_scaled)[0]\n",
    "    prob = model.predict_proba(features_scaled)[0][prediction]\n",
    "\n",
    "    labels = {0: \"Not Depressed 🙂\", 1: \"Depressed 😔\"}\n",
    "\n",
    "    print(f\"Prediction: {labels[prediction]}  (confidence: {prob:.2f})\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94743b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: Not Depressed 🙂  (confidence: 0.56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shris\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\utils\\validation.py:2749: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "test_file = \"D:/depressiondetector/hello4.wav\"\n",
    "predict_audio(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dadab3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
