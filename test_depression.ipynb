{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d2185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\shris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\shris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import sys\n",
    "!{sys.executable} -m pip install --quiet sentence-transformers transformers datasets textblob vaderSentiment imbalanced-learn xgboost tqdm joblib\n",
    "\n",
    "# %%\n",
    "import os, glob, re, joblib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from xgboost import XGBClassifier\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57a555dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6838808ca7f547e5bb4e5e5e72d36f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c43bf60b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 163, Dev: 56, Test: 56\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "BASE_DATA_DIR = r\"D:/depressiondetector/diag-woz\"\n",
    "train_labels_path = r\"D:/depressiondetector/labels/train_split.csv\"\n",
    "dev_labels_path   = r\"D:/depressiondetector/labels/dev_split.csv\"\n",
    "test_labels_path  = r\"D:/depressiondetector/labels/test_split.csv\"\n",
    "\n",
    "def normalize_label(df):\n",
    "    rename_map = {\n",
    "        'PHQ_Binary': 'PHQ8_Binary',\n",
    "        'phq8_binary': 'PHQ8_Binary',\n",
    "        'PHQ8_binary': 'PHQ8_Binary'\n",
    "    }\n",
    "    df.rename(columns={c: rename_map[c] for c in rename_map if c in df.columns}, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_labels = normalize_label(pd.read_csv(train_labels_path))\n",
    "dev_labels   = normalize_label(pd.read_csv(dev_labels_path))\n",
    "test_labels  = normalize_label(pd.read_csv(test_labels_path))\n",
    "\n",
    "for df in [train_labels, dev_labels, test_labels]:\n",
    "    df['Participant_ID'] = df['Participant_ID'].astype(str)\n",
    "\n",
    "print(f\"Train: {len(train_labels)}, Dev: {len(dev_labels)}, Test: {len(test_labels)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4f199d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bbdd1f5253a453485e521e830faa5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading transcripts:   0%|          | 0/275 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 275 transcripts\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "def load_transcript_csv(pid, base_dir=BASE_DATA_DIR):\n",
    "    pid = str(pid)\n",
    "    folder = os.path.join(base_dir, f\"{pid}_P\")\n",
    "    csv_files = glob.glob(os.path.join(folder, \"**\", \"*.csv\"), recursive=True)\n",
    "    if not csv_files:\n",
    "        return \"\"\n",
    "    \n",
    "    transcript_files = [f for f in csv_files if 'transcript' in os.path.basename(f).lower()]\n",
    "    best = transcript_files[0] if transcript_files else sorted(csv_files, key=lambda x: os.path.getsize(x), reverse=True)[0]\n",
    "    df = pd.read_csv(best)\n",
    "    \n",
    "    text_cols = [c for c in df.columns if 'text' in c.lower()]\n",
    "    if not text_cols:\n",
    "        return \"\"\n",
    "    \n",
    "    text_col = text_cols[0]\n",
    "    if \"Speaker\" in df.columns:\n",
    "        df = df[df[\"Speaker\"].astype(str).str.lower().isin([\"participant\",\"user\",\"patient\",\"client\"])]\n",
    "    return \" \".join(df[text_col].astype(str).tolist())\n",
    "\n",
    "# %%\n",
    "all_ids = sorted(set(train_labels['Participant_ID'].tolist() + \n",
    "                     dev_labels['Participant_ID'].tolist() + \n",
    "                     test_labels['Participant_ID'].tolist()))\n",
    "\n",
    "rows = []\n",
    "for pid in tqdm(all_ids, desc=\"Loading transcripts\"):\n",
    "    rows.append({\"Participant_ID\": pid, \"raw_text\": load_transcript_csv(pid)})\n",
    "\n",
    "df_text = pd.DataFrame(rows)\n",
    "print(f\"Loaded {len(df_text)} transcripts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "083954ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "df_text[\"clean_text\"] = df_text[\"raw_text\"].apply(clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "010324ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "a = df_text[df_text[\"Participant_ID\"]==\"308\"][\"clean_text\"]\n",
    "print(len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca672a6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>so I'm going to  interview in Spanish  okay  g...</td>\n",
       "      <td>so i'm going to interview in spanish okay good...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>yeah there's also on Craigslist so that's why ...</td>\n",
       "      <td>yeah there's also on craigslist so that's why ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>just move around a little bit  when you're fin...</td>\n",
       "      <td>just move around a little bit when you're fini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>wow okay  when you're finished when she's done...</td>\n",
       "      <td>wow okay when you're finished when she's done ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>so we'll just move around a little bit tonight...</td>\n",
       "      <td>so we'll just move around a little bit tonight...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant_ID                                           raw_text  \\\n",
       "0            300  so I'm going to  interview in Spanish  okay  g...   \n",
       "1            301  yeah there's also on Craigslist so that's why ...   \n",
       "2            302  just move around a little bit  when you're fin...   \n",
       "3            303  wow okay  when you're finished when she's done...   \n",
       "4            304  so we'll just move around a little bit tonight...   \n",
       "\n",
       "                                          clean_text  \n",
       "0  so i'm going to interview in spanish okay good...  \n",
       "1  yeah there's also on craigslist so that's why ...  \n",
       "2  just move around a little bit when you're fini...  \n",
       "3  wow okay when you're finished when she's done ...  \n",
       "4  so we'll just move around a little bit tonight...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb0d831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting linguistic features...\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "print(\"Extracting linguistic features...\")\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "def extract_linguistic_features(text):\n",
    "    words = text.lower().split()\n",
    "    word_count = len(words)\n",
    "    if word_count == 0:\n",
    "        return [0] * 18  # Changed from 15 to 18\n",
    "\n",
    "    first_person = ['i', 'me', 'my', 'mine', 'myself']\n",
    "    neg_words = ['sad','depressed','lonely','hopeless','worthless','tired','empty','numb',\n",
    "                 'anxious','worried','scared','pain','hurt','alone','crying','awful','terrible',\n",
    "                 'miserable','helpless','useless','broken']\n",
    "    absolutist = ['always','never','nothing','everything','completely','totally','absolutely','entire','all','every','none']\n",
    "    \n",
    "    # âš ï¸ CRITICAL: Suicide/Self-Harm Keywords\n",
    "    suicide_words = ['suicide','suicidal','kill myself','end my life','die','death','harm myself',\n",
    "                     'cut myself','overdose','jump','hang myself','gun','pills','razor']\n",
    "\n",
    "    first_person_ratio = sum(1 for w in words if w in first_person) / word_count\n",
    "    neg_count = sum(1 for w in words if w in neg_words)\n",
    "    neg_ratio = neg_count / word_count\n",
    "    absolutist_count = sum(1 for w in words if w in absolutist)\n",
    "    absolutist_ratio = absolutist_count / word_count\n",
    "    \n",
    "    # **NEW: Suicide detection**\n",
    "    text_lower = text.lower()\n",
    "    suicide_count = sum(1 for sw in suicide_words if sw in text_lower)\n",
    "    suicide_flag = 1 if suicide_count > 0 else 0\n",
    "\n",
    "    blob = TextBlob(text)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    vader_scores = vader.polarity_scores(text)\n",
    "\n",
    "    lexical_diversity = len(set(words)) / word_count if word_count > 0 else 0\n",
    "    avg_word_len = np.mean([len(w) for w in words]) if words else 0\n",
    "\n",
    "    return [\n",
    "        word_count, first_person_ratio, neg_count, neg_ratio, absolutist_count, absolutist_ratio,\n",
    "        polarity, subjectivity, vader_scores['neg'], vader_scores['neu'], vader_scores['pos'],\n",
    "        vader_scores['compound'], lexical_diversity, avg_word_len, text.count('?'),\n",
    "        suicide_count, suicide_flag, text_lower.count('!')  # NEW FEATURES\n",
    "    ]\n",
    "\n",
    "ling_features = df_text['clean_text'].apply(extract_linguistic_features)\n",
    "ling_df = pd.DataFrame(ling_features.tolist(), columns=[\n",
    "    'word_count','first_person_ratio','neg_word_count','neg_ratio','absolutist_count','absolutist_ratio',\n",
    "    'polarity','subjectivity','vader_neg','vader_neu','vader_pos','vader_compound','lexical_diversity',\n",
    "    'avg_word_len','question_count',\n",
    "    'suicide_count','suicide_flag','exclamation_count'  # NEW\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93986382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>first_person_ratio</th>\n",
       "      <th>neg_word_count</th>\n",
       "      <th>neg_ratio</th>\n",
       "      <th>absolutist_count</th>\n",
       "      <th>absolutist_ratio</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>question_count</th>\n",
       "      <th>suicide_count</th>\n",
       "      <th>suicide_flag</th>\n",
       "      <th>exclamation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>322</td>\n",
       "      <td>0.096273</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0.015528</td>\n",
       "      <td>0.240335</td>\n",
       "      <td>0.512849</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.555901</td>\n",
       "      <td>4.068323</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1399</td>\n",
       "      <td>0.096497</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>17</td>\n",
       "      <td>0.012152</td>\n",
       "      <td>0.119045</td>\n",
       "      <td>0.604685</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.268763</td>\n",
       "      <td>3.961401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>609</td>\n",
       "      <td>0.054187</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>5</td>\n",
       "      <td>0.008210</td>\n",
       "      <td>0.167731</td>\n",
       "      <td>0.521223</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.481117</td>\n",
       "      <td>4.096880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1916</td>\n",
       "      <td>0.061065</td>\n",
       "      <td>5</td>\n",
       "      <td>0.002610</td>\n",
       "      <td>9</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>0.218301</td>\n",
       "      <td>0.544658</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.241649</td>\n",
       "      <td>3.914927</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>992</td>\n",
       "      <td>0.082661</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>6</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>0.174526</td>\n",
       "      <td>0.525062</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.363911</td>\n",
       "      <td>4.021169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_count  first_person_ratio  neg_word_count  neg_ratio  \\\n",
       "0         322            0.096273               0   0.000000   \n",
       "1        1399            0.096497               1   0.000715   \n",
       "2         609            0.054187               1   0.001642   \n",
       "3        1916            0.061065               5   0.002610   \n",
       "4         992            0.082661               1   0.001008   \n",
       "\n",
       "   absolutist_count  absolutist_ratio  polarity  subjectivity  vader_neg  \\\n",
       "0                 5          0.015528  0.240335      0.512849      0.056   \n",
       "1                17          0.012152  0.119045      0.604685      0.075   \n",
       "2                 5          0.008210  0.167731      0.521223      0.058   \n",
       "3                 9          0.004697  0.218301      0.544658      0.045   \n",
       "4                 6          0.006048  0.174526      0.525062      0.069   \n",
       "\n",
       "   vader_neu  vader_pos  vader_compound  lexical_diversity  avg_word_len  \\\n",
       "0      0.620      0.324          0.9989           0.555901      4.068323   \n",
       "1      0.740      0.184          0.9996           0.268763      3.961401   \n",
       "2      0.700      0.243          0.9994           0.481117      4.096880   \n",
       "3      0.742      0.213          0.9999           0.241649      3.914927   \n",
       "4      0.744      0.187          0.9990           0.363911      4.021169   \n",
       "\n",
       "   question_count  suicide_count  suicide_flag  exclamation_count  \n",
       "0               0              0             0                  0  \n",
       "1               0              0             0                  0  \n",
       "2               0              0             0                  0  \n",
       "3               0              1             1                  0  \n",
       "4               0              0             0                  0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ling_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "175dba5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Mental Health BERT model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name mental/mental-bert-base-uncased. Creating a new one with mean pooling.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at mental/mental-bert-base-uncased and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5be5c04e39b45bb8dc6a12ac3d0831d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature dimension: 786\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading Mental Health BERT model...\")\n",
    "MODEL_NAME = \"mental/mental-bert-base-uncased\"  # After you get access\n",
    "\n",
    "\n",
    "sbert = SentenceTransformer(MODEL_NAME)\n",
    "\n",
    "embeddings = sbert.encode(\n",
    "    df_text[\"clean_text\"].tolist(),\n",
    "    batch_size=16,\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "emb_dim = embeddings.shape[1]\n",
    "emb_cols = [f\"emb_{i}\" for i in range(emb_dim)]\n",
    "\n",
    "df_emb = pd.DataFrame(embeddings, columns=emb_cols)\n",
    "df_emb[\"Participant_ID\"] = df_text[\"Participant_ID\"]\n",
    "\n",
    "# Merge embeddings + linguistic features\n",
    "df_final = df_text.merge(df_emb, on=\"Participant_ID\")\n",
    "df_final = pd.concat([df_final.reset_index(drop=True), ling_df], axis=1)\n",
    "\n",
    "print(f\"Final feature dimension: {len(emb_cols) + len(ling_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bd03847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(emb_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8d1aa0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant_ID</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>emb_0</th>\n",
       "      <th>emb_1</th>\n",
       "      <th>emb_2</th>\n",
       "      <th>emb_3</th>\n",
       "      <th>emb_4</th>\n",
       "      <th>emb_5</th>\n",
       "      <th>emb_6</th>\n",
       "      <th>...</th>\n",
       "      <th>vader_neg</th>\n",
       "      <th>vader_neu</th>\n",
       "      <th>vader_pos</th>\n",
       "      <th>vader_compound</th>\n",
       "      <th>lexical_diversity</th>\n",
       "      <th>avg_word_len</th>\n",
       "      <th>question_count</th>\n",
       "      <th>suicide_count</th>\n",
       "      <th>suicide_flag</th>\n",
       "      <th>exclamation_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>300</td>\n",
       "      <td>so I'm going to  interview in Spanish  okay  g...</td>\n",
       "      <td>so i'm going to interview in spanish okay good...</td>\n",
       "      <td>-0.102613</td>\n",
       "      <td>0.023676</td>\n",
       "      <td>0.175381</td>\n",
       "      <td>-0.071310</td>\n",
       "      <td>0.256039</td>\n",
       "      <td>-0.174053</td>\n",
       "      <td>-0.059907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.324</td>\n",
       "      <td>0.9989</td>\n",
       "      <td>0.555901</td>\n",
       "      <td>4.068323</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>yeah there's also on Craigslist so that's why ...</td>\n",
       "      <td>yeah there's also on craigslist so that's why ...</td>\n",
       "      <td>-0.067665</td>\n",
       "      <td>-0.019338</td>\n",
       "      <td>0.076278</td>\n",
       "      <td>-0.085264</td>\n",
       "      <td>0.166650</td>\n",
       "      <td>-0.169355</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.9996</td>\n",
       "      <td>0.268763</td>\n",
       "      <td>3.961401</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>302</td>\n",
       "      <td>just move around a little bit  when you're fin...</td>\n",
       "      <td>just move around a little bit when you're fini...</td>\n",
       "      <td>-0.142839</td>\n",
       "      <td>0.018652</td>\n",
       "      <td>0.144626</td>\n",
       "      <td>-0.067519</td>\n",
       "      <td>0.281602</td>\n",
       "      <td>-0.209833</td>\n",
       "      <td>-0.073179</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.243</td>\n",
       "      <td>0.9994</td>\n",
       "      <td>0.481117</td>\n",
       "      <td>4.096880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>303</td>\n",
       "      <td>wow okay  when you're finished when she's done...</td>\n",
       "      <td>wow okay when you're finished when she's done ...</td>\n",
       "      <td>-0.125157</td>\n",
       "      <td>-0.037072</td>\n",
       "      <td>0.094689</td>\n",
       "      <td>-0.131592</td>\n",
       "      <td>0.223761</td>\n",
       "      <td>-0.188919</td>\n",
       "      <td>0.046842</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.742</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.9999</td>\n",
       "      <td>0.241649</td>\n",
       "      <td>3.914927</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>304</td>\n",
       "      <td>so we'll just move around a little bit tonight...</td>\n",
       "      <td>so we'll just move around a little bit tonight...</td>\n",
       "      <td>-0.246813</td>\n",
       "      <td>-0.090445</td>\n",
       "      <td>0.103634</td>\n",
       "      <td>-0.090123</td>\n",
       "      <td>0.246010</td>\n",
       "      <td>-0.044509</td>\n",
       "      <td>0.008121</td>\n",
       "      <td>...</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.9990</td>\n",
       "      <td>0.363911</td>\n",
       "      <td>4.021169</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 789 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Participant_ID                                           raw_text  \\\n",
       "0            300  so I'm going to  interview in Spanish  okay  g...   \n",
       "1            301  yeah there's also on Craigslist so that's why ...   \n",
       "2            302  just move around a little bit  when you're fin...   \n",
       "3            303  wow okay  when you're finished when she's done...   \n",
       "4            304  so we'll just move around a little bit tonight...   \n",
       "\n",
       "                                          clean_text     emb_0     emb_1  \\\n",
       "0  so i'm going to interview in spanish okay good... -0.102613  0.023676   \n",
       "1  yeah there's also on craigslist so that's why ... -0.067665 -0.019338   \n",
       "2  just move around a little bit when you're fini... -0.142839  0.018652   \n",
       "3  wow okay when you're finished when she's done ... -0.125157 -0.037072   \n",
       "4  so we'll just move around a little bit tonight... -0.246813 -0.090445   \n",
       "\n",
       "      emb_2     emb_3     emb_4     emb_5     emb_6  ...  vader_neg  \\\n",
       "0  0.175381 -0.071310  0.256039 -0.174053 -0.059907  ...      0.056   \n",
       "1  0.076278 -0.085264  0.166650 -0.169355  0.001108  ...      0.075   \n",
       "2  0.144626 -0.067519  0.281602 -0.209833 -0.073179  ...      0.058   \n",
       "3  0.094689 -0.131592  0.223761 -0.188919  0.046842  ...      0.045   \n",
       "4  0.103634 -0.090123  0.246010 -0.044509  0.008121  ...      0.069   \n",
       "\n",
       "   vader_neu  vader_pos  vader_compound  lexical_diversity  avg_word_len  \\\n",
       "0      0.620      0.324          0.9989           0.555901      4.068323   \n",
       "1      0.740      0.184          0.9996           0.268763      3.961401   \n",
       "2      0.700      0.243          0.9994           0.481117      4.096880   \n",
       "3      0.742      0.213          0.9999           0.241649      3.914927   \n",
       "4      0.744      0.187          0.9990           0.363911      4.021169   \n",
       "\n",
       "   question_count  suicide_count  suicide_flag  exclamation_count  \n",
       "0               0              0             0                  0  \n",
       "1               0              0             0                  0  \n",
       "2               0              0             0                  0  \n",
       "3               0              1             1                  0  \n",
       "4               0              0             0                  0  \n",
       "\n",
       "[5 rows x 789 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f95d4c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (163, 786), Dev: (56, 786), Test: (56, 786)\n"
     ]
    }
   ],
   "source": [
    "feature_cols = emb_cols + ling_df.columns.tolist()\n",
    "\n",
    "train = train_labels.merge(df_final, on=\"Participant_ID\", how=\"inner\")\n",
    "dev   = dev_labels.merge(df_final, on=\"Participant_ID\", how=\"inner\")\n",
    "test  = test_labels.merge(df_final, on=\"Participant_ID\", how=\"inner\")\n",
    "\n",
    "X_train, y_train = train[feature_cols].values, train[\"PHQ8_Binary\"].values\n",
    "X_dev,   y_dev   = dev[feature_cols].values,   dev[\"PHQ8_Binary\"].values\n",
    "X_test,  y_test  = test[feature_cols].values,  test[\"PHQ8_Binary\"].values\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Dev: {X_dev.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695303d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a26d0a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original class distribution: [126  37]\n",
      "After SMOTE: (252, 786)\n",
      "Class distribution: [126 126]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_dev_s   = scaler.transform(X_dev)\n",
    "X_test_s  = scaler.transform(X_test)\n",
    "\n",
    "# Better SMOTE with lower k_neighbors for small minority class\n",
    "print(f\"Original class distribution: {np.bincount(y_train.astype(int))}\")\n",
    "smote = SMOTE(random_state=42, k_neighbors=min(3, (y_train == 1).sum() - 1))\n",
    "X_res, y_res = smote.fit_resample(X_train_s, y_train)\n",
    "\n",
    "print(f\"After SMOTE: {X_res.shape}\")\n",
    "print(f\"Class distribution: {np.bincount(y_res.astype(int))}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7b11f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c761dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training models with class balancing...\n",
      "Scale pos weight: 1.00\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "print(\"Training models with class balancing...\")\n",
    "\n",
    "# Logistic Regression with balanced weights\n",
    "lr = LogisticRegression(max_iter=3000, random_state=42, class_weight='balanced', C=0.1)\n",
    "lr.fit(X_res, y_res)\n",
    "\n",
    "# Random Forest with balanced weights\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300, \n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=4,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=42, \n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_res, y_res)\n",
    "\n",
    "# Gradient Boosting\n",
    "gb = GradientBoostingClassifier(\n",
    "    n_estimators=200, \n",
    "    learning_rate=0.05, \n",
    "    max_depth=3,\n",
    "    min_samples_split=10,\n",
    "    random_state=42\n",
    ")\n",
    "gb.fit(X_res, y_res)\n",
    "\n",
    "# XGBoost with scale_pos_weight\n",
    "scale_pos_weight = (y_res == 0).sum() / (y_res == 1).sum()\n",
    "print(f\"Scale pos weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=300, \n",
    "    learning_rate=0.05, \n",
    "    max_depth=4,\n",
    "    min_child_weight=3,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='logloss',\n",
    "    random_state=42\n",
    ")\n",
    "xgb.fit(X_res, y_res)\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eb7dcff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate(model, X, y, name):\n",
    "    pred = model.predict(X)\n",
    "    prob = model.predict_proba(X)[:,1]\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Accuracy: {accuracy_score(y, pred):.4f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc_score(y, prob):.4f}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(confusion_matrix(y, pred))\n",
    "    print(f\"\\nClassification Report:\")\n",
    "    print(classification_report(y, pred, target_names=['Not Depressed', 'Depressed']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad52272c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DEVELOPMENT SET RESULTS\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Logistic Regression\n",
      "==================================================\n",
      "Accuracy: 0.8036\n",
      "ROC-AUC: 0.7027\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  3]\n",
      " [ 8  4]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed       0.84      0.93      0.88        44\n",
      "    Depressed       0.57      0.33      0.42        12\n",
      "\n",
      "     accuracy                           0.80        56\n",
      "    macro avg       0.70      0.63      0.65        56\n",
      " weighted avg       0.78      0.80      0.78        56\n",
      "\n",
      "\n",
      "==================================================\n",
      "Random Forest\n",
      "==================================================\n",
      "Accuracy: 0.7679\n",
      "ROC-AUC: 0.6080\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  3]\n",
      " [10  2]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed       0.80      0.93      0.86        44\n",
      "    Depressed       0.40      0.17      0.24        12\n",
      "\n",
      "     accuracy                           0.77        56\n",
      "    macro avg       0.60      0.55      0.55        56\n",
      " weighted avg       0.72      0.77      0.73        56\n",
      "\n",
      "\n",
      "==================================================\n",
      "Gradient Boosting\n",
      "==================================================\n",
      "Accuracy: 0.7500\n",
      "ROC-AUC: 0.5833\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  4]\n",
      " [10  2]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed       0.80      0.91      0.85        44\n",
      "    Depressed       0.33      0.17      0.22        12\n",
      "\n",
      "     accuracy                           0.75        56\n",
      "    macro avg       0.57      0.54      0.54        56\n",
      " weighted avg       0.70      0.75      0.72        56\n",
      "\n",
      "\n",
      "==================================================\n",
      "XGBoost\n",
      "==================================================\n",
      "Accuracy: 0.7500\n",
      "ROC-AUC: 0.6458\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  5]\n",
      " [ 9  3]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed       0.81      0.89      0.85        44\n",
      "    Depressed       0.38      0.25      0.30        12\n",
      "\n",
      "     accuracy                           0.75        56\n",
      "    macro avg       0.59      0.57      0.57        56\n",
      " weighted avg       0.72      0.75      0.73        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEVELOPMENT SET RESULTS\")\n",
    "print(\"=\"*60)\n",
    "evaluate(lr, X_dev_s, y_dev, \"Logistic Regression\")\n",
    "evaluate(rf, X_dev_s, y_dev, \"Random Forest\")\n",
    "evaluate(gb, X_dev_s, y_dev, \"Gradient Boosting\")\n",
    "evaluate(xgb, X_dev_s, y_dev, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccdaa221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TEST SET RESULTS (FINAL)\n",
      "============================================================\n",
      "\n",
      "==================================================\n",
      "Logistic Regression\n",
      "==================================================\n",
      "Accuracy: 0.6429\n",
      "ROC-AUC: 0.6154\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33  6]\n",
      " [14  3]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed       0.70      0.85      0.77        39\n",
      "    Depressed       0.33      0.18      0.23        17\n",
      "\n",
      "     accuracy                           0.64        56\n",
      "    macro avg       0.52      0.51      0.50        56\n",
      " weighted avg       0.59      0.64      0.60        56\n",
      "\n",
      "\n",
      "==================================================\n",
      "Random Forest\n",
      "==================================================\n",
      "Accuracy: 0.6964\n",
      "ROC-AUC: 0.7195\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36  3]\n",
      " [14  3]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed       0.72      0.92      0.81        39\n",
      "    Depressed       0.50      0.18      0.26        17\n",
      "\n",
      "     accuracy                           0.70        56\n",
      "    macro avg       0.61      0.55      0.53        56\n",
      " weighted avg       0.65      0.70      0.64        56\n",
      "\n",
      "\n",
      "==================================================\n",
      "Gradient Boosting\n",
      "==================================================\n",
      "Accuracy: 0.6964\n",
      "ROC-AUC: 0.6893\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33  6]\n",
      " [11  6]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed       0.75      0.85      0.80        39\n",
      "    Depressed       0.50      0.35      0.41        17\n",
      "\n",
      "     accuracy                           0.70        56\n",
      "    macro avg       0.62      0.60      0.60        56\n",
      " weighted avg       0.67      0.70      0.68        56\n",
      "\n",
      "\n",
      "==================================================\n",
      "XGBoost\n",
      "==================================================\n",
      "Accuracy: 0.6786\n",
      "ROC-AUC: 0.7059\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34  5]\n",
      " [13  4]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "Not Depressed       0.72      0.87      0.79        39\n",
      "    Depressed       0.44      0.24      0.31        17\n",
      "\n",
      "     accuracy                           0.68        56\n",
      "    macro avg       0.58      0.55      0.55        56\n",
      " weighted avg       0.64      0.68      0.64        56\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET RESULTS (FINAL)\")\n",
    "print(\"=\"*60)\n",
    "evaluate(lr, X_test_s, y_test, \"Logistic Regression\")\n",
    "evaluate(rf, X_test_s, y_test, \"Random Forest\")\n",
    "evaluate(gb, X_test_s, y_test, \"Gradient Boosting\")\n",
    "evaluate(xgb, X_test_s, y_test, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a96551d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Models saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "os.makedirs(\"text_depression_models\", exist_ok=True)\n",
    "\n",
    "# Save all models\n",
    "joblib.dump(xgb, \"text_depression_models/xgb_model.joblib\")\n",
    "joblib.dump(rf, \"text_depression_models/rf_model.joblib\")\n",
    "joblib.dump(lr, \"text_depression_models/lr_model.joblib\")\n",
    "joblib.dump(gb, \"text_depression_models/gb_model.joblib\")\n",
    "joblib.dump(scaler, \"text_depression_models/scaler.joblib\")\n",
    "joblib.dump(sbert, \"text_depression_models/sbert_model.joblib\")\n",
    "\n",
    "print(\"âœ… Models saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0a41c7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_depression(text, threshold=0.35):\n",
    "    \"\"\"Predict depression from raw text with adjustable threshold\"\"\"\n",
    "    clean_text = clean(text)\n",
    "    ling_feats = extract_linguistic_features(clean_text)\n",
    "    emb = sbert.encode([clean_text])\n",
    "    \n",
    "    features = np.concatenate([emb[0], ling_feats])\n",
    "    features = scaler.transform([features])\n",
    "    \n",
    "    # Use XGBoost with custom threshold\n",
    "    prob = xgb.predict_proba(features)[0]\n",
    "    pred = 1 if prob[1] >= threshold else 0\n",
    "    \n",
    "    result = \"Depressed ðŸ˜”\" if pred == 1 else \"Not Depressed ðŸ™‚\"\n",
    "    confidence = prob[1] if pred == 1 else prob[0]\n",
    "    \n",
    "    return f\"{result} | Confidence: {confidence:.2%} | Depression Probability: {prob[1]:.2%}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10cbf1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    # text = re.sub(r\"http\\S+|www\\.\\S+\", \" \", text)\n",
    "    # text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1eebf998",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_risk_levels(text):\n",
    "    \"\"\"Predict with interpretable risk levels + SUICIDE OVERRIDE\"\"\"\n",
    "    clean_text = clean(text)\n",
    "    ling_feats = extract_linguistic_features(clean_text)\n",
    "    emb = sbert.encode([clean_text])\n",
    "    \n",
    "    features = np.concatenate([emb[0], ling_feats])\n",
    "    features = scaler.transform([features])\n",
    "    \n",
    "    prob = xgb.predict_proba(features)[0]\n",
    "    \n",
    "    # **CRITICAL OVERRIDE: If suicide keywords detected, force HIGH RISK**\n",
    "    suicide_keywords = ['suicide', 'kill myself', 'end my life', 'want to die', 'gonna die']\n",
    "    text_lower = text.lower()\n",
    "    has_suicide = any(kw in text_lower for kw in suicide_keywords)\n",
    "    \n",
    "    if has_suicide:\n",
    "        prob[1] = max(prob[1], 0.75)  # Force at least 75% depression probability\n",
    "    \n",
    "    # Risk stratification\n",
    "    if prob[1] >= 0.50 or has_suicide:\n",
    "        result = \"ðŸ”´ SEVERE Depression Risk - URGENT HELP NEEDED\"\n",
    "    elif prob[1] >= 0.35:\n",
    "        result = \"ðŸŸ  HIGH Depression Risk\"\n",
    "    elif prob[1] >= 0.15:\n",
    "        result = \"ðŸŸ¡ MODERATE Risk\"\n",
    "    elif prob[1] >= 0.05:\n",
    "        result = \"ðŸŸ¢ LOW Risk\"\n",
    "    else:\n",
    "        result = \"âœ… Minimal Risk\"\n",
    "    \n",
    "    if has_suicide:\n",
    "        result += \" âš ï¸ SUICIDE IDEATION DETECTED\"\n",
    "    \n",
    "    return f\"{result} | Depression Score: {prob[1]:.1%}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fd43416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_with_risk_levels(text):\n",
    "#     \"\"\"Predict with interpretable risk levels\"\"\"\n",
    "#     clean_text = clean(text)\n",
    "#     ling_feats = extract_linguistic_features(clean_text)\n",
    "#     emb = sbert.encode([clean_text])\n",
    "    \n",
    "#     features = np.concatenate([emb[0], ling_feats])\n",
    "#     features = scaler.transform([features])\n",
    "    \n",
    "#     prob = lr.predict_proba(features)[0]\n",
    "    \n",
    "#     # Risk stratification\n",
    "#     if prob[1] >= 0.50:\n",
    "#         result = \"ðŸ”´ SEVERE Depression Risk\"\n",
    "#     elif prob[1] >= 0.35:\n",
    "#         result = \"ðŸŸ  HIGH Depression Risk\"\n",
    "#     elif prob[1] >= 0.15:\n",
    "#         result = \"ðŸŸ¡ MODERATE Risk\"\n",
    "#     elif prob[1] >= 0.05:\n",
    "#         result = \"ðŸŸ¢ LOW Risk\"\n",
    "#     else:\n",
    "#         result = \"âœ… Minimal Risk\"\n",
    "    \n",
    "#     return f\"{result} | Depression Score: {prob[1]:.1%}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4a9027a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_with_risk_levels(text):\n",
    "#     \"\"\"Predict with interpretable risk levels\"\"\"\n",
    "#     clean_text = clean(text)\n",
    "#     ling_feats = extract_linguistic_features(clean_text)\n",
    "#     emb = sbert.encode([clean_text])\n",
    "    \n",
    "#     features = np.concatenate([emb[0], ling_feats])\n",
    "#     features = scaler.transform([features])\n",
    "    \n",
    "#     prob = rf.predict_proba(features)[0]\n",
    "    \n",
    "#     # Risk stratification\n",
    "#     if prob[1] >= 0.50:\n",
    "#         result = \"ðŸ”´ SEVERE Depression Risk\"\n",
    "#     elif prob[1] >= 0.35:\n",
    "#         result = \"ðŸŸ  HIGH Depression Risk\"\n",
    "#     elif prob[1] >= 0.15:\n",
    "#         result = \"ðŸŸ¡ MODERATE Risk\"\n",
    "#     elif prob[1] >= 0.05:\n",
    "#         result = \"ðŸŸ¢ LOW Risk\"\n",
    "#     else:\n",
    "#         result = \"âœ… Minimal Risk\"\n",
    "    \n",
    "#     return f\"{result} | Depression Score: {prob[1]:.1%}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "6d62d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_with_risk_levels(text):\n",
    "#     \"\"\"Predict with interpretable risk levels\"\"\"\n",
    "#     clean_text = clean(text)\n",
    "#     ling_feats = extract_linguistic_features(clean_text)\n",
    "#     emb = sbert.encode([clean_text])\n",
    "    \n",
    "#     features = np.concatenate([emb[0], ling_feats])\n",
    "#     features = scaler.transform([features])\n",
    "    \n",
    "#     prob = gb.predict_proba(features)[0]\n",
    "    \n",
    "#     # Risk stratification\n",
    "#     if prob[1] >= 0.50:\n",
    "#         result = \"ðŸ”´ SEVERE Depression Risk\"\n",
    "#     elif prob[1] >= 0.35:\n",
    "#         result = \"ðŸŸ  HIGH Depression Risk\"\n",
    "#     elif prob[1] >= 0.15:\n",
    "#         result = \"ðŸŸ¡ MODERATE Risk\"\n",
    "#     elif prob[1] >= 0.05:\n",
    "#         result = \"ðŸŸ¢ LOW Risk\"\n",
    "#     else:\n",
    "#         result = \"âœ… Minimal Risk\"\n",
    "    \n",
    "#     return f\"{result} | Depression Score: {prob[1]:.1%}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0402e636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”´ SEVERE Depression Risk - URGENT HELP NEEDED âš ï¸ SUICIDE IDEATION DETECTED | Depression Score: 99.1%\n"
     ]
    }
   ],
   "source": [
    "test_text = \"I don't know if I have what it takes to continue to do  I survived day by day  TriNet  if I can  have you been diagnosed with depression  how long ago were you diagnosed  4 years ago  4 years ago couple years ago  what got you to seek help  my mood was just not right  I was always feeling down and depressed and lack of energy always wanted  lack of Interest  I've gone to therapy it has been useful for me in the past is it useful for me now I don't  I would love to talk to someone I just feel like I don't have anyone  I don't depend on anyone  yeah I'm sorry  did you think you had a problem before you found out  oh yeah I've always felt depressed on my life  what were your symptoms  dr. Banerjee  why is sleep a lot lack of Interest  my appetite was uncontrollable either lack of or I will should mean gluttonous and eating the wrong things  have you noticed any changes in your behavior or thoughts lately  can you give me an example of that  my sister just passed away  I'm sorry to hear that all right to go  Robbie Keane  homeless and jobless 2 weeks ago  I got to a point where  I just missed her and I want to be with her  yeah  how close are you to your family  pretty Dysfunctional Family  really cat depend on them  I just depend on myself  that sounds really hard  it is what it is what are you going to do  yeah  I'd like to give up but  my parents just buried their daughter six months ago they don't want to bury their other daughter  what advice would you give yourself 10 or 20 years ago  I just haven't had good luck  the corporate manager for 20 years  who would have known 20 years ago what advice I would have given myself again  try to commit suicide to return back to work  no it's just too rough trying to pick up all the pieces  together  I don't know what advice I would give myself  when I started with my corporate job.  I didn't know I'd be here  yeah  when is the last time you felt really happy  really  a couple days ago this week  I've been trying to make ends meet  got a lot of things sprouting  got a lot of things  that are happening for me but I need like two more weeks here in LA  I can connect the dots but  I'm actually I don't have what it takes to stay here for two more weeks  so  I was happy because I was getting all back  things are starting to look up for me but it's just time again is it on  my car is packed and I'm hitting the road  how do your best friend describe you  Royal  emotional  you consider yourself an introvert  I have been lately  I was an extrovert  20 years  what made you decide to do that  circumstances in my life I don't feel is confit  okay  which one of your most memorable experiences  I don't know it's hard to say  can't really answer that one  okay  okay I think I've asked everything I need to  thanks for sharing your thoughts with me thank you goodbye\"\n",
    "print(predict_with_risk_levels(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f05fe676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Minimal Risk | Depression Score: 0.9%\n"
     ]
    }
   ],
   "source": [
    "test_text2 = \"I had a great day today! Went for a walk and met up with friends. Feeling really good about life.\"\n",
    "print(predict_with_risk_levels(test_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a47d7972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŸ  HIGH Depression Risk | Depression Score: 41.7%\n"
     ]
    }
   ],
   "source": [
    "test_text2 = \"\"\"\n",
    "I feel completely hopeless and worthless. Nothing ever goes right for me. \n",
    "I'm always tired and can't find joy in anything anymore. I feel so alone \n",
    "and empty inside. I don't see the point in trying anymore. Everything feels \n",
    "like too much effort. I just want to sleep all the time and never wake up.\n",
    "\"\"\"\n",
    "print(predict_with_risk_levels(test_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43403bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”´ SEVERE Depression Risk - URGENT HELP NEEDED âš ï¸ SUICIDE IDEATION DETECTED | Depression Score: 75.0%\n"
     ]
    }
   ],
   "source": [
    "test_text2 = \"I had a bad day today,and i am gonna suicide if life goes this way as my college pressurises me every odd they take a huge lump of fees on name of giving good placements but when it comes to giving placemnets they have nothing to give!! \"\n",
    "print(predict_with_risk_levels(test_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7fa8d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
